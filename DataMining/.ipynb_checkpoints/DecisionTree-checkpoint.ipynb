{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "nums 9\n",
      "num1 1\n",
      "num2 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3522138890491458 \n",
      " 0.15104444572649994\n",
      "0.5032583347756457\n"
     ]
    }
   ],
   "source": [
    "# 决策树\n",
    "\n",
    "# Entropy计算\n",
    "'''\n",
    "@ param : class_nums\n",
    "'''\n",
    "nums = float(input('nums'))\n",
    "num1 = float(input('num1'))\n",
    "num2 = float(input('num2'))\n",
    "# print(num1/nums, '\\n',num2/nums)\n",
    "\n",
    "e1 = 0 - (num1/nums)*math.log(num1/nums,2.0)\n",
    "e2 = 0 - (num2/nums)*math.log(num2/nums,2.0)\n",
    "print(e1, '\\n',e2)\n",
    "\n",
    "Entropy = e1 + e2\n",
    "print(Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "nums 14\n",
      "numa1 5\n",
      "numa2 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1:0.4591836734693877\n"
     ]
    }
   ],
   "source": [
    "# Gini 系数\n",
    "nums_a = float(input('nums'))\n",
    "num_a1 = float(input('numa1'))\n",
    "num_a2 = float(input('numa2'))\n",
    "\n",
    "# nums_b = float(input('nums_b'))\n",
    "# num_b1 = float(input('num_b1'))\n",
    "# num_b2 = float(input('num_b2'))\n",
    "# print(num1/nums, '\\n',num2/nums)\n",
    "\n",
    "g1 = 1 - (num_a1/nums_a)**2 - (num_a2/nums_a)**2\n",
    "print('g1:{}'.format(g1))\n",
    "# g2 = 1 - (num_b1/nums_b)**2 - (num_b2/nums_b)**2\n",
    "\n",
    "# sum = nums_a + nums_b\n",
    "\n",
    "# Gini = /sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.dataset = self.createDataset()\n",
    "\n",
    "    def createDataset(self):\n",
    "        \"\"\"创建数据集\"\"\"\n",
    "        dataSet = [[1, 1, 'yes'],\n",
    "                   [1, 1, 'yes'],\n",
    "                   [1, 0, 'no'],\n",
    "                   [0, 1, 'no'],\n",
    "                   [0, 1, 'no']]\n",
    "        labels = ['no surfacing', 'flippers']\n",
    "        # change to discrete values\n",
    "        return dataSet, labels\n",
    "\n",
    "    def calcShannonEnt(self):\n",
    "        \"\"\"计算信息熵\"\"\"\n",
    "        # 计算数据集中实例的总数。\n",
    "        numEntries = len(self.dataset)\n",
    "        # 创建一个数据字典，它的键值是最后一列的数值\n",
    "        labelCounts = {}\n",
    "        for featVec in self.dataset:\n",
    "            currentLabel = featVec[-1]\n",
    "            # 每个键值都记录了当前类别出现的次数\n",
    "            if currentLabel not in labelCounts.keys():\n",
    "                labelCounts[currentLabel] = 0\n",
    "            labelCounts[currentLabel] += 1\n",
    "        shannonEnt = 0.0\n",
    "        for key in labelCounts:\n",
    "            # 使用所有类标签的发生频率计算类别出现的概率\n",
    "            prob = float(labelCounts[key]) / numEntries\n",
    "            # 将用这个概率计算香农熵\n",
    "            shannonEnt -= prob * log(prob, 2)\n",
    "        return shannonEnt\n",
    "\n",
    "    def splitDataset(self, axis, value):\n",
    "        \"\"\"\n",
    "        :param axis: 划分数据集的特征\n",
    "        :param value: 需要返回的特征的值\n",
    "        \"\"\"\n",
    "        # 创建一个新的列表对象\n",
    "        retDataset = []\n",
    "        for featVec in self.dataset:\n",
    "            # 按照某个特征划分数据集时，就需要将所有符合要求的元素抽取出来\n",
    "            if featVec[axis] == value:\n",
    "                reducedFeatVec = featVec[:axis]\n",
    "                reducedFeatVec.extend(featVec[axis+1:])\n",
    "                retDataset.append(reducedFeatVec)\n",
    "        return retDataset\n",
    "\n",
    "    def chooseBestFeatureToSplit(self):\n",
    "        numFeatures = len(self.dataset[0]) - 1\n",
    "        baseEntropy = self.calcShannonEnt()\n",
    "        bestInfoGain = 0.0; bestFeature = -1\n",
    "        for i in range(numFeatures):\n",
    "            featList = [example[i] for example in self.dataset]\n",
    "            uniqueVals = set(featList)  # 创建唯一的分类标签列表\n",
    "            newEntropy = 0.0\n",
    "            # 计算每种划分方式的信息熵\n",
    "            for value in uniqueVals:\n",
    "                subDataSet = self.splitDataset(i, value)\n",
    "                prob = len(subDataSet) / float(len(self.dataset))\n",
    "                self.dataset = subDataSet\n",
    "                newEntropy += prob * self.calcShannonEnt()\n",
    "            infoGain = baseEntropy - newEntropy\n",
    "            # 计算最好的信息增益\n",
    "            if infoGain > bestInfoGain:\n",
    "                bestInfoGain = infoGain\n",
    "                bestFeature = i\n",
    "        return bestFeature\n",
    "\n",
    "    def majorityCnt(self, classList: \"分类名称的列表\"):\n",
    "        # 创建键值为classList中唯一值的数据字典\n",
    "        classCount = {}\n",
    "        # 字典对象存储了classList中每个类标签出现的频率，最后利用operator操作键值排序字典，并返回出现次数最多的分类名称。\n",
    "        for vote in classList:\n",
    "            if vote not in classCount.keys():\n",
    "                classCount[vote] = 0\n",
    "            classCount[vote] += 1\n",
    "        sortedClassCount = sorted(classCount.items(), key=lambda x:x[1], reverse=True)\n",
    "        return sortedClassCount[0][0]\n",
    "\n",
    "    def createTree(self, labels):\n",
    "        # 先创建了名为classList的列表变量，其中包含了数据集的所有类标签\n",
    "        classList = [example[-1] for example in self.dataset]\n",
    "        # 递归函数的第一个停止条件是所有的类标签完全相同，则直接返回该类标签\n",
    "        if classList.count(classList[0]) == len(classList):\n",
    "            return classList[0]\n",
    "        # 递归函数的第二个停止条件是使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组\n",
    "        if len(self.dataset) == 1:\n",
    "            # 由于第二个条件无法简单地返回唯一的类标签，这里使用majorityCnt函数挑选出现次数最多的类别作为返回值。\n",
    "            return self.majorityCnt(classList)\n",
    "        # 当前数据集选取的最好特征存储在变量bestFeat中，得到列表包含的所有属性值\n",
    "        bestFeat = self.chooseBestFeatureToSplit(self.dataset)\n",
    "        bestFeatLabel = labels[bestFeat]\n",
    "        # 字典变量myTree存储了树的所有信息\n",
    "        myTree = {bestFeatLabel:{}}\n",
    "        del labels[bestFeat]\n",
    "        \"\"\"\n",
    "        遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数\n",
    "        createTree()，得到的返回值将被插入到字典变量myTree中，因此函数终止执行时，字典中将\n",
    "        会嵌套很多代表叶子节点信息的字典数据\n",
    "        \"\"\"\n",
    "        featValues = [example[bestFeat] for example in self.dataset]\n",
    "        uniqueVals = set(featValues)\n",
    "        for value in uniqueVals:\n",
    "            # 复制类标签\n",
    "            subLabels = labels[:]\n",
    "            \"\"\"\n",
    "            因为在Python语言中函数参数是列表类型时，参数是按照引用方式传递的。为了保\n",
    "            证每次调用函数createTree()时不改变原始列表的内容，使用新变量subLabels代替原始列表。\n",
    "            \"\"\"\n",
    "            myTree[bestFeatLabel][value] = self.createTree(self.splitDataset(\n",
    "                bestFeat, value\n",
    "            ), subLabels)\n",
    "        return myTree\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        return self.createTree(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''创建数据集'''\n",
    "        self.dataset,self.label = self.createDataset()\n",
    "        # 特征属性的个数\n",
    "        self.numOfFeature = len(self.dataset[0])\n",
    "        #print('numOfFeature:{}'.format(self.numOfFeature))\n",
    "        # 数据集中实例数\n",
    "        self.numOfEntities = len(self.dataset) \n",
    "        \n",
    "        \n",
    "    def createDataset(self):\n",
    "        '''数据集参考书上p123'''\n",
    "        dataset = [\n",
    "            [1,0,'+'],\n",
    "            [1,1,'+'],\n",
    "            [1,1,'+'],\n",
    "            [1,0,'-']\n",
    "        ]\n",
    "        # 相当于第一行的表头\n",
    "        label = ['A','B','res']\n",
    "        return dataset, label\n",
    "    \n",
    "    def calcEntropy(self):\n",
    "        '''计算信息熵'''\n",
    "        # 用字典记录不同属性出现的次数\n",
    "        ClassesCounts = {}\n",
    "        for i in self.dataset:\n",
    "            # print(i)\n",
    "            # 保存当前记录的所属分类\n",
    "            current_class = i[-1]\n",
    "            if current_class not in ClassesCounts.keys():\n",
    "                ClassesCounts[current_class] = 0\n",
    "            ClassesCounts[current_class] += 1\n",
    "        # 信息熵\n",
    "        Entropy = 0.0\n",
    "        for key in ClassesCounts:\n",
    "            # 计算每类属性出现的概率并计算信息熵\n",
    "            prob = float(ClassesCounts[key]) / self.numOfEntities\n",
    "            Entropy -= prob * math.log(prob, 2)\n",
    "        \n",
    "        return Entropy\n",
    "    \n",
    "    # 划分数据集\n",
    "    def splitDataset(self, axis, value):\n",
    "        '''\n",
    "        @ param axis: 划分数据集的特征\n",
    "        @ param value: 需要返回特征的值\n",
    "        '''\n",
    "        pass\n",
    "        retDataset = []\n",
    "        for i in self.dataset:\n",
    "            if i[axis] == value:\n",
    "                pass\n",
    "    \n",
    "    def chooseBestAttributeToSplit(self):\n",
    "        '''选择一个最佳划分'''\n",
    "        # 计算基础信息熵\n",
    "        base_entropy = self.calcEntropy()\n",
    "        bastInfoGain = 0.0\n",
    "        bestFeature = -1\n",
    "        for i in range(self.numOfFeature):\n",
    "            for example in self.dataset:\n",
    "                print('ex{}对应的example{}'.format(i,example))\n",
    "                print('exi:{}'.format(example[i]))\n",
    "#             feature_list = [example[i] for example in self.dataset]\n",
    "        # print('feature_list:{}'.format(feature_list))\n",
    "\n",
    "    def createTree(self):\n",
    "        '''建立决策树'''\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex0对应的example[1, 0, '+']\n",
      "exi:1\n",
      "ex0对应的example[1, 1, '+']\n",
      "exi:1\n",
      "ex0对应的example[1, 1, '+']\n",
      "exi:1\n",
      "ex0对应的example[1, 0, '-']\n",
      "exi:1\n",
      "ex1对应的example[1, 0, '+']\n",
      "exi:0\n",
      "ex1对应的example[1, 1, '+']\n",
      "exi:1\n",
      "ex1对应的example[1, 1, '+']\n",
      "exi:1\n",
      "ex1对应的example[1, 0, '-']\n",
      "exi:0\n",
      "ex2对应的example[1, 0, '+']\n",
      "exi:+\n",
      "ex2对应的example[1, 1, '+']\n",
      "exi:+\n",
      "ex2对应的example[1, 1, '+']\n",
      "exi:+\n",
      "ex2对应的example[1, 0, '-']\n",
      "exi:-\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree()\n",
    "\n",
    "tree.calcEntropy()\n",
    "tree.chooseBestAttributeToSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
